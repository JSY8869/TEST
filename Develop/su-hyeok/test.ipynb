{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('develop': conda)"
  },
  "interpreter": {
   "hash": "2d96fff6999327154348f938a106912098fae993114b52ef6bc3b6f818b91f59"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "from functools import reduce\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE1 = os.path.join(\"doing_data_train.txt\")\n",
    "TEST_FILE1 = os.path.join(\"doing_data_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 은경이는 밥을 먹었습니다.\n2 필웅이는 산책을 갔습니다.\n3 필웅이는 뭘했어? \t산책\t2\n4 수종이는 당구를 쳤습니다.\n5 경임이는 볼링을 쳤습니다.\n6 수종이는 뭘했어? \t당구\t4\n7 은경이는 일을 했습니다.\n8 경임이는 축구를 했습니다.\n9 수종이는 뭘했어? \t당구\t4\n10 필웅이는 잠을 잤습니다.\n11 수종이는 야구를 했습니다\n12 수종이는 뭘했어? \t야구\t11\n13 은경이는 데이트를 했습니다\n14 은경이는 골프를 쳤습니다.\n15 경임이는 뭘했어? \t축구\t8\n1 은경이는 밥을 먹었습니다.\n2 필웅이는 산책을 갔습니다.\n3 필웅이는 뭘했어? \t산책\t2\n4 수종이는 당구를 쳤습니다.\n5 경임이는 볼링을 쳤습니다.\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "lines = open(TRAIN_FILE1 , \"rb\")\n",
    "for line in lines:\n",
    "    line = line.decode(\"utf-8\").strip()\n",
    "    i = i + 1\n",
    "    print(line)\n",
    "    if i == 20:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dir):\n",
    "    stories, questions, answers = [], [], [] # 각각 스토리, 질문, 답변을 저장할 예정\n",
    "    story_temp = [] # 현재 시점의 스토리 임시 저장\n",
    "    lines = open(dir, \"rb\")\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.decode(\"utf-8\") # b' 제거\n",
    "        line = line.strip() # '\\n' 제거\n",
    "        idx, text = line.split(\" \", 1) # 맨 앞에 있는 id number 분리\n",
    "        # 여기까지는 모든 줄에 적용되는 전처리\n",
    "\n",
    "        if int(idx) == 1:\n",
    "            story_temp = []\n",
    "\n",
    "        if \"\\t\" in text: # 현재 읽는 줄이 질문 (tab) 답변 (tab)인 경우\n",
    "            question, answer, _ = text.split(\"\\t\") # 질문과 답변을 각각 저장\n",
    "            stories.append([x for x in story_temp if x]) # 지금까지의 누적 스토리를 스토리에 저장\n",
    "            questions.append(question)\n",
    "            answers.append(answer)\n",
    "\n",
    "        else: # 현재 읽는 줄이 스토리인 경우\n",
    "            story_temp.append(text) # 임시 저장\n",
    "\n",
    "    lines.close()\n",
    "    return stories, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = read_data(TRAIN_FILE1)\n",
    "test_data1 = read_data(TEST_FILE1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stories, train_questions, train_answers = read_data(TRAIN_FILE1)\n",
    "test_stories, test_questions, test_answers = read_data(TEST_FILE1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "훈련용 스토리의 개수 : 3600\n훈련용 질문의 개수 : 3600\n훈련용 답변의 개수 : 3600\n테스트용 스토리의 개수 : 3600\n테스트용 질문의 개수 : 3600\n테스트용 답변의 개수 : 3600\n"
     ]
    }
   ],
   "source": [
    "print('훈련용 스토리의 개수 :', len(train_stories))\n",
    "print('훈련용 질문의 개수 :',len(train_questions))\n",
    "print('훈련용 답변의 개수 :',len(train_answers))\n",
    "print('테스트용 스토리의 개수 :',len(test_stories))\n",
    "print('테스트용 질문의 개수 :',len(test_questions))\n",
    "print('테스트용 답변의 개수 :',len(test_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data):\n",
    "    counter = FreqDist()\n",
    "\n",
    "    # 두 문장의 story를 하나의 문장으로 통합하는 함수\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "\n",
    "    # 각 샘플의 길이를 저장하는 리스트\n",
    "    story_len = []\n",
    "    question_len = []\n",
    "\n",
    "    for stories, questions, answers in [train_data1, test_data1]:\n",
    "        for story in stories:\n",
    "            stories = tokenize(flatten(story)) # 스토리의 문장들을 펼친 후 토큰화\n",
    "            story_len.append(len(stories)) # 각 story의 길이 저장\n",
    "            for word in stories: # 단어 집합에 단어 추가\n",
    "                counter[word] += 1\n",
    "        for question in questions:\n",
    "            question = tokenize(question)\n",
    "            question_len.append(len(question))\n",
    "            for word in question:\n",
    "                counter[word] += 1\n",
    "        for answer in answers:\n",
    "            answer = tokenize(answer)\n",
    "            for word in answer:\n",
    "                counter[word] += 1\n",
    "\n",
    "    # 단어 집합 생성\n",
    "    word2idx = {word : (idx + 1) for idx, (word, _) in enumerate(counter.most_common())}\n",
    "    idx2word = {idx : word for word, idx in word2idx.items()}\n",
    "\n",
    "    # 가장 긴 샘플의 길이\n",
    "    story_max_len = np.max(story_len)\n",
    "    question_max_len = np.max(question_len)\n",
    "\n",
    "    return word2idx, idx2word, story_max_len, question_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\shlee\\anaconda3\\envs\\develop\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "word2idx, idx2word, story_max_len, question_max_len = preprocess_data(train_data1, test_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "스토리의 최대 길이 : 37\n질문의 최대 길이 : 3\n"
     ]
    }
   ],
   "source": [
    "print('스토리의 최대 길이 :',story_max_len)\n",
    "print('질문의 최대 길이 :',question_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data, word2idx, story_maxlen, question_maxlen):\n",
    "    Xs, Xq, Y = [], [], []\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "\n",
    "    stories, questions, answers = data\n",
    "    for story, question, answer in zip(stories, questions, answers):\n",
    "        xs = [word2idx[w] for w in tokenize(flatten(story))]\n",
    "        xq = [word2idx[w] for w in tokenize(question)]\n",
    "        Xs.append(xs)\n",
    "        Xq.append(xq)\n",
    "        Y.append(word2idx[answer])\n",
    "        # 스토리와 질문은 각각의 최대 길이로 패딩\n",
    "        # 정답은 원-핫 인코딩\n",
    "    return pad_sequences(Xs, maxlen=story_maxlen),\\\n",
    "           pad_sequences(Xq, maxlen=question_maxlen),\\\n",
    "           to_categorical(Y, num_classes=len(word2idx) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\shlee\\anaconda3\\envs\\develop\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "Xstrain1, Xqtrain1, Ytrain1 = vectorize(train_data1, word2idx, story_max_len, question_max_len)\n",
    "Xstest1, Xqtest1, Ytest1 = vectorize(test_data1, word2idx, story_max_len, question_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3600, 37) (3600, 3) (3600, 99) (3600, 37) (3600, 3) (3600, 99)\n"
     ]
    }
   ],
   "source": [
    "print(Xstrain1.shape, Xqtrain1.shape, Ytrain1.shape, Xstest1.shape, Xqtest1.shape, Ytest1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Permute, dot, add, concatenate\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에포크 횟수\n",
    "train_epochs = 120\n",
    "# 배치 크기\n",
    "batch_size = 32\n",
    "# 임베딩 크기\n",
    "embed_size = 50\n",
    "# LSTM의 크기\n",
    "lstm_size = 64\n",
    "# 과적합 방지 기법인 드롭아웃 적용 비율\n",
    "dropout_rate = 0.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Stories : KerasTensor(type_spec=TensorSpec(shape=(None, 37), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\")\nQuestion: KerasTensor(type_spec=TensorSpec(shape=(None, 3), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\")\n"
     ]
    }
   ],
   "source": [
    "input_sequence = Input((story_max_len,))\n",
    "question = Input((question_max_len,))\n",
    "\n",
    "print('Stories :', input_sequence)\n",
    "print('Question:', question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스토리를 위한 첫번째 임베딩. 그림에서의 Embedding A\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=embed_size))\n",
    "input_encoder_m.add(Dropout(dropout_rate))\n",
    "# 결과 : (samples, story_max_len, embedding_dim) / 샘플의 수, 문장의 최대 길이, 임베딩 벡터의 차원\n",
    "\n",
    "# 스토리를 위한 두번째 임베딩. 그림에서의 Embedding C\n",
    "# 임베딩 벡터의 차원을 question_max_len(질문의 최대 길이)로 한다.\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=question_max_len))\n",
    "input_encoder_c.add(Dropout(dropout_rate))\n",
    "# 결과 : (samples, story_max_len, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이(임베딩 벡터의 차원)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문을 위한 임베딩. 그림에서의 Embedding B\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=embed_size,\n",
    "                               input_length=question_max_len))\n",
    "question_encoder.add(Dropout(dropout_rate))\n",
    "# 결과 : (samples, question_max_len, embedding_dim) / 샘플의 수, 질문의 최대 길이, 임베딩 벡터의 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input encoded m KerasTensor(type_spec=TensorSpec(shape=(None, 37, 50), dtype=tf.float32, name=None), name='sequential/dropout/Identity:0', description=\"created by layer 'sequential'\")\nInput encoded c KerasTensor(type_spec=TensorSpec(shape=(None, 37, 3), dtype=tf.float32, name=None), name='sequential_1/dropout_1/Identity:0', description=\"created by layer 'sequential_1'\")\nQuestion encoded KerasTensor(type_spec=TensorSpec(shape=(None, 3, 50), dtype=tf.float32, name=None), name='sequential_2/dropout_2/Identity:0', description=\"created by layer 'sequential_2'\")\n"
     ]
    }
   ],
   "source": [
    "# 실질적인 임베딩 과정\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "\n",
    "print('Input encoded m', input_encoded_m)\n",
    "print('Input encoded c', input_encoded_c)\n",
    "print('Question encoded', question_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Match shape KerasTensor(type_spec=TensorSpec(shape=(None, 37, 3), dtype=tf.float32, name=None), name='activation/Softmax:0', description=\"created by layer 'activation'\")\nResponse shape KerasTensor(type_spec=TensorSpec(shape=(None, 3, 37), dtype=tf.float32, name=None), name='permute/transpose:0', description=\"created by layer 'permute'\")\nAnswer shape KerasTensor(type_spec=TensorSpec(shape=(None, 3, 87), dtype=tf.float32, name=None), name='concatenate/concat:0', description=\"created by layer 'concatenate'\")\n"
     ]
    }
   ],
   "source": [
    "# 스토리 단어들과 질문 단어들 간의 유사도를 구하는 과정\n",
    "# 유사도는 내적을 사용한다.\n",
    "match = dot([input_encoded_m, question_encoded], axes=-1, normalize=False)\n",
    "match = Activation('softmax')(match)\n",
    "print('Match shape', match)\n",
    "# 결과 : (samples, story_maxlen, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이\n",
    "\n",
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, input_encoded_c])  # (samples, story_max_len, question_max_len)\n",
    "response = Permute((2, 1))(response)  # (samples, question_max_len, story_max_len)\n",
    "print('Response shape', response)\n",
    "\n",
    "# concatenate the response vector with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])\n",
    "print('Answer shape', answer)\n",
    "\n",
    "answer = LSTM(lstm_size)(answer)  # Generate tensors of shape 32\n",
    "answer = Dropout(dropout_rate)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "_________________________\n",
      "input_2 (InputLayer)            [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, None, 50)     4950        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 3, 50)        4950        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 37, 3)        0           sequential[0][0]                 \n",
      "                                                                 sequential_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 37, 3)        0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, None, 3)      297         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 37, 3)        0           activation[0][0]                 \n",
      "                                                                 sequential_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (None, 3, 37)        0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 3, 87)        0           permute[0][0]                    \n",
      "                                                                 sequential_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 64)           38912       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 99)           6435        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 99)           0           dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 55,544\n",
      "Trainable params: 55,544\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n",
      "113/113 [==============================] - 4s 14ms/step - loss: 3.7709 - acc: 0.0500 - val_loss: 3.7180 - val_acc: 0.1167\n",
      "Epoch 2/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 2.8868 - acc: 0.1456 - val_loss: 2.8796 - val_acc: 0.1500\n",
      "Epoch 3/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 2.2331 - acc: 0.2178 - val_loss: 2.5794 - val_acc: 0.3000\n",
      "Epoch 4/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 1.6538 - acc: 0.4961 - val_loss: 2.1674 - val_acc: 0.5167\n",
      "Epoch 5/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 0.9280 - acc: 0.7928 - val_loss: 1.8053 - val_acc: 0.7000\n",
      "Epoch 6/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 0.4357 - acc: 0.9369 - val_loss: 1.6752 - val_acc: 0.7500\n",
      "Epoch 7/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 0.1886 - acc: 0.9861 - val_loss: 1.6490 - val_acc: 0.7500\n",
      "Epoch 8/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 0.0716 - acc: 0.9978 - val_loss: 1.7433 - val_acc: 0.7500\n",
      "Epoch 9/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 0.0314 - acc: 0.9992 - val_loss: 1.8446 - val_acc: 0.7500\n",
      "Epoch 10/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 0.0127 - acc: 1.0000 - val_loss: 2.0191 - val_acc: 0.7500\n",
      "Epoch 11/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 2.1115 - val_acc: 0.7500\n",
      "Epoch 12/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 2.2819 - val_acc: 0.7500\n",
      "Epoch 13/120\n",
      "113/113 [==============================] - 1s 9ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.3481 - val_acc: 0.7500\n",
      "Epoch 14/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 8.1856e-04 - acc: 1.0000 - val_loss: 2.4766 - val_acc: 0.7500\n",
      "Epoch 15/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 3.8572e-04 - acc: 1.0000 - val_loss: 2.6125 - val_acc: 0.7500\n",
      "Epoch 16/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 3.0602e-04 - acc: 1.0000 - val_loss: 2.6149 - val_acc: 0.7500\n",
      "Epoch 17/120\n",
      "113/113 [==============================] - 1s 9ms/step - loss: 2.9654e-04 - acc: 1.0000 - val_loss: 2.6581 - val_acc: 0.7333\n",
      "Epoch 18/120\n",
      "113/113 [==============================] - 1s 9ms/step - loss: 1.0355e-04 - acc: 1.0000 - val_loss: 2.7571 - val_acc: 0.7333\n",
      "Epoch 19/120\n",
      "113/113 [==============================] - 1s 9ms/step - loss: 6.6029e-05 - acc: 1.0000 - val_loss: 2.8345 - val_acc: 0.7500\n",
      "Epoch 20/120\n",
      "113/113 [==============================] - 1s 9ms/step - loss: 5.4857e-05 - acc: 1.0000 - val_loss: 2.9637 - val_acc: 0.7500\n",
      "Epoch 21/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 3.7594e-05 - acc: 1.0000 - val_loss: 2.9302 - val_acc: 0.7333\n",
      "Epoch 22/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 2.6102e-05 - acc: 1.0000 - val_loss: 3.0736 - val_acc: 0.7333\n",
      "Epoch 23/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 1.9742e-05 - acc: 1.0000 - val_loss: 3.1015 - val_acc: 0.7500\n",
      "Epoch 24/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 1.3175e-05 - acc: 1.0000 - val_loss: 3.1274 - val_acc: 0.7500\n",
      "Epoch 25/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 9.7218e-06 - acc: 1.0000 - val_loss: 3.2104 - val_acc: 0.7333\n",
      "Epoch 26/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 7.7349e-06 - acc: 1.0000 - val_loss: 3.1774 - val_acc: 0.7333\n",
      "Epoch 27/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 5.4600e-06 - acc: 1.0000 - val_loss: 3.2924 - val_acc: 0.7333\n",
      "Epoch 28/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 4.5654e-06 - acc: 1.0000 - val_loss: 3.2898 - val_acc: 0.7333\n",
      "Epoch 29/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 4.2882e-06 - acc: 1.0000 - val_loss: 3.4000 - val_acc: 0.7333\n",
      "Epoch 30/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 3.7032e-06 - acc: 1.0000 - val_loss: 3.4257 - val_acc: 0.7500\n",
      "Epoch 31/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 2.8344e-06 - acc: 1.0000 - val_loss: 3.4985 - val_acc: 0.7333\n",
      "Epoch 32/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 4.7840e-06 - acc: 1.0000 - val_loss: 3.4350 - val_acc: 0.7333\n",
      "Epoch 33/120\n",
      "113/113 [==============================] - 1s 9ms/step - loss: 2.3750e-06 - acc: 1.0000 - val_loss: 3.5526 - val_acc: 0.7333\n",
      "Epoch 34/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 1.6324e-06 - acc: 1.0000 - val_loss: 3.4955 - val_acc: 0.7333\n",
      "Epoch 35/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 2.5298e-06 - acc: 1.0000 - val_loss: 3.5642 - val_acc: 0.7667\n",
      "Epoch 36/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 3.5161e-06 - acc: 1.0000 - val_loss: 3.6085 - val_acc: 0.7500\n",
      "Epoch 37/120\n",
      "113/113 [==============================] - 1s 9ms/step - loss: 1.0479e-06 - acc: 1.0000 - val_loss: 3.6492 - val_acc: 0.7333\n",
      "Epoch 38/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 1.2627e-06 - acc: 1.0000 - val_loss: 3.6978 - val_acc: 0.7333\n",
      "Epoch 39/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 1.0338e-06 - acc: 1.0000 - val_loss: 3.6932 - val_acc: 0.7333\n",
      "Epoch 40/120\n",
      "113/113 [==============================] - 1s 9ms/step - loss: 8.4161e-07 - acc: 1.0000 - val_loss: 3.6972 - val_acc: 0.7500\n",
      "Epoch 41/120\n",
      "113/113 [==============================] - 1s 12ms/step - loss: 8.3481e-07 - acc: 1.0000 - val_loss: 3.7409 - val_acc: 0.7333\n",
      "Epoch 42/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 7.5201e-07 - acc: 1.0000 - val_loss: 3.6921 - val_acc: 0.7333\n",
      "Epoch 43/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 6.3823e-07 - acc: 1.0000 - val_loss: 3.7343 - val_acc: 0.7500\n",
      "Epoch 44/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 7.2205e-07 - acc: 1.0000 - val_loss: 3.6954 - val_acc: 0.7667\n",
      "Epoch 45/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 5.9610e-07 - acc: 1.0000 - val_loss: 3.7225 - val_acc: 0.7500\n",
      "Epoch 46/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 6.9309e-07 - acc: 1.0000 - val_loss: 3.8175 - val_acc: 0.7500\n",
      "Epoch 47/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 6.1719e-07 - acc: 1.0000 - val_loss: 3.7468 - val_acc: 0.7500\n",
      "Epoch 48/120\n",
      "113/113 [==============================] - 1s 9ms/step - loss: 1.5311e-06 - acc: 1.0000 - val_loss: 3.7599 - val_acc: 0.7500\n",
      "Epoch 49/120\n",
      "113/113 [==============================] - 1s 9ms/step - loss: 4.6209e-07 - acc: 1.0000 - val_loss: 3.8072 - val_acc: 0.7500\n",
      "Epoch 50/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 5.8902e-07 - acc: 1.0000 - val_loss: 3.7363 - val_acc: 0.7333\n",
      "Epoch 51/120\n",
      "113/113 [==============================] - 1s 8ms/step - loss: 4.1540e-07 - acc: 1.0000 - val_loss: 3.8095 - val_acc: 0.7500\n",
      "Epoch 52/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 4.4744e-07 - acc: 1.0000 - val_loss: 3.8417 - val_acc: 0.7500\n",
      "Epoch 53/120\n",
      "113/113 [==============================] - 1s 12ms/step - loss: 8.3707e-07 - acc: 1.0000 - val_loss: 3.8269 - val_acc: 0.7500\n",
      "Epoch 54/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 4.7291e-07 - acc: 1.0000 - val_loss: 3.8499 - val_acc: 0.7333\n",
      "Epoch 55/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 3.1729e-07 - acc: 1.0000 - val_loss: 3.8753 - val_acc: 0.7333\n",
      "Epoch 56/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 3.1287e-07 - acc: 1.0000 - val_loss: 3.8297 - val_acc: 0.7333\n",
      "Epoch 57/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 4.2059e-07 - acc: 1.0000 - val_loss: 3.8338 - val_acc: 0.7333\n",
      "Epoch 58/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 1.6938e-07 - acc: 1.0000 - val_loss: 3.9656 - val_acc: 0.7333\n",
      "Epoch 59/120\n",
      "113/113 [==============================] - 1s 12ms/step - loss: 3.3644e-07 - acc: 1.0000 - val_loss: 3.9459 - val_acc: 0.7333\n",
      "Epoch 60/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 2.1606e-07 - acc: 1.0000 - val_loss: 3.9417 - val_acc: 0.7333\n",
      "Epoch 61/120\n",
      "113/113 [==============================] - 1s 13ms/step - loss: 2.9686e-07 - acc: 1.0000 - val_loss: 3.9773 - val_acc: 0.7333\n",
      "Epoch 62/120\n",
      "113/113 [==============================] - 1s 12ms/step - loss: 8.2487e-07 - acc: 1.0000 - val_loss: 3.9529 - val_acc: 0.7333\n",
      "Epoch 63/120\n",
      "113/113 [==============================] - 1s 13ms/step - loss: 3.7555e-07 - acc: 1.0000 - val_loss: 3.9207 - val_acc: 0.7500\n",
      "Epoch 64/120\n",
      "113/113 [==============================] - 1s 12ms/step - loss: 2.5905e-07 - acc: 1.0000 - val_loss: 3.8993 - val_acc: 0.7333\n",
      "Epoch 65/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 3.7536e-07 - acc: 1.0000 - val_loss: 3.8838 - val_acc: 0.7500\n",
      "Epoch 66/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 3.6035e-07 - acc: 1.0000 - val_loss: 3.9684 - val_acc: 0.7500\n",
      "Epoch 67/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 8.1434e-07 - acc: 1.0000 - val_loss: 3.9316 - val_acc: 0.7500\n",
      "Epoch 68/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 3.5705e-07 - acc: 1.0000 - val_loss: 3.9445 - val_acc: 0.7333\n",
      "Epoch 69/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 8.0163e-07 - acc: 1.0000 - val_loss: 3.9424 - val_acc: 0.7500\n",
      "Epoch 70/120\n",
      "113/113 [==============================] - 1s 12ms/step - loss: 2.0712e-07 - acc: 1.0000 - val_loss: 3.9616 - val_acc: 0.7500\n",
      "Epoch 71/120\n",
      "113/113 [==============================] - 1s 12ms/step - loss: 3.3474e-07 - acc: 1.0000 - val_loss: 3.9882 - val_acc: 0.7333\n",
      "Epoch 72/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 1.6610e-07 - acc: 1.0000 - val_loss: 3.9549 - val_acc: 0.7333\n",
      "Epoch 73/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 7.4091e-07 - acc: 1.0000 - val_loss: 3.8152 - val_acc: 0.7500\n",
      "Epoch 74/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 2.1679e-07 - acc: 1.0000 - val_loss: 3.8995 - val_acc: 0.7500\n",
      "Epoch 75/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 1.5222e-07 - acc: 1.0000 - val_loss: 3.9740 - val_acc: 0.7500\n",
      "Epoch 76/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 1.4567e-07 - acc: 1.0000 - val_loss: 3.9692 - val_acc: 0.7500\n",
      "Epoch 77/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 2.3838e-07 - acc: 1.0000 - val_loss: 4.0063 - val_acc: 0.7667\n",
      "Epoch 78/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 5.5997e-07 - acc: 1.0000 - val_loss: 3.9284 - val_acc: 0.7500\n",
      "Epoch 79/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 1.2365e-07 - acc: 1.0000 - val_loss: 3.9655 - val_acc: 0.7500\n",
      "Epoch 80/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 1.5865e-07 - acc: 1.0000 - val_loss: 3.9781 - val_acc: 0.7500\n",
      "Epoch 81/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 1.8341e-07 - acc: 1.0000 - val_loss: 3.8993 - val_acc: 0.7500\n",
      "Epoch 82/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 1.4431e-07 - acc: 1.0000 - val_loss: 3.8942 - val_acc: 0.7500\n",
      "Epoch 83/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 1.2100e-07 - acc: 1.0000 - val_loss: 3.9264 - val_acc: 0.7500\n",
      "Epoch 84/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 3.0339e-07 - acc: 1.0000 - val_loss: 3.9876 - val_acc: 0.7500\n",
      "Epoch 85/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 1.8255e-07 - acc: 1.0000 - val_loss: 4.0127 - val_acc: 0.7500\n",
      "Epoch 86/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 1.3729e-07 - acc: 1.0000 - val_loss: 3.9821 - val_acc: 0.7500\n",
      "Epoch 87/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 2.5231e-07 - acc: 1.0000 - val_loss: 3.9884 - val_acc: 0.7500\n",
      "Epoch 88/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 4.4784e-07 - acc: 1.0000 - val_loss: 3.9310 - val_acc: 0.7333\n",
      "Epoch 89/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 9.3976e-08 - acc: 1.0000 - val_loss: 3.9934 - val_acc: 0.7333\n",
      "Epoch 90/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 1.7520e-07 - acc: 1.0000 - val_loss: 4.0568 - val_acc: 0.7333\n",
      "Epoch 91/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 1.1133e-07 - acc: 1.0000 - val_loss: 4.0746 - val_acc: 0.7333\n",
      "Epoch 92/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 9.4241e-08 - acc: 1.0000 - val_loss: 4.0589 - val_acc: 0.7333\n",
      "Epoch 93/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 1.2199e-07 - acc: 1.0000 - val_loss: 4.0347 - val_acc: 0.7500\n",
      "Epoch 94/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 1.5987e-07 - acc: 1.0000 - val_loss: 4.0162 - val_acc: 0.7500\n",
      "Epoch 95/120\n",
      "113/113 [==============================] - 1s 9ms/step - loss: 1.1116e-07 - acc: 1.0000 - val_loss: 4.0439 - val_acc: 0.7500\n",
      "Epoch 96/120\n",
      "113/113 [==============================] - 1s 9ms/step - loss: 1.0219e-07 - acc: 1.0000 - val_loss: 4.0788 - val_acc: 0.7500\n",
      "Epoch 97/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 2.4847e-07 - acc: 1.0000 - val_loss: 4.0077 - val_acc: 0.7500\n",
      "Epoch 98/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 1.6933e-07 - acc: 1.0000 - val_loss: 4.0477 - val_acc: 0.7500\n",
      "Epoch 99/120\n",
      "113/113 [==============================] - 1s 9ms/step - loss: 1.0368e-07 - acc: 1.0000 - val_loss: 4.0804 - val_acc: 0.7333\n",
      "Epoch 100/120\n",
      "113/113 [==============================] - 1s 9ms/step - loss: 9.7188e-08 - acc: 1.0000 - val_loss: 4.1092 - val_acc: 0.7333\n",
      "Epoch 101/120\n",
      "113/113 [==============================] - 1s 9ms/step - loss: 7.2419e-08 - acc: 1.0000 - val_loss: 4.0903 - val_acc: 0.7333\n",
      "Epoch 102/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 9.4870e-08 - acc: 1.0000 - val_loss: 4.1060 - val_acc: 0.7333\n",
      "Epoch 103/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 1.1623e-07 - acc: 1.0000 - val_loss: 4.1072 - val_acc: 0.7333\n",
      "Epoch 104/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 1.7007e-07 - acc: 1.0000 - val_loss: 4.1582 - val_acc: 0.7333\n",
      "Epoch 105/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 1.6159e-07 - acc: 1.0000 - val_loss: 4.1480 - val_acc: 0.7333\n",
      "Epoch 106/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 1.0123e-07 - acc: 1.0000 - val_loss: 4.0984 - val_acc: 0.7333\n",
      "Epoch 107/120\n",
      "113/113 [==============================] - 1s 9ms/step - loss: 6.1459e-08 - acc: 1.0000 - val_loss: 4.1185 - val_acc: 0.7333\n",
      "Epoch 108/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 9.2287e-08 - acc: 1.0000 - val_loss: 4.1493 - val_acc: 0.7333\n",
      "Epoch 109/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 6.2287e-08 - acc: 1.0000 - val_loss: 4.1064 - val_acc: 0.7333\n",
      "Epoch 110/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 1.7725e-07 - acc: 1.0000 - val_loss: 4.1022 - val_acc: 0.7333\n",
      "Epoch 111/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 1.0682e-07 - acc: 1.0000 - val_loss: 4.1147 - val_acc: 0.7333\n",
      "Epoch 112/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 9.5530e-08 - acc: 1.0000 - val_loss: 4.0610 - val_acc: 0.7333\n",
      "Epoch 113/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 7.6139e-07 - acc: 1.0000 - val_loss: 4.0566 - val_acc: 0.7333\n",
      "Epoch 114/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 9.0201e-08 - acc: 1.0000 - val_loss: 4.1313 - val_acc: 0.7500\n",
      "Epoch 115/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 5.7353e-08 - acc: 1.0000 - val_loss: 4.1342 - val_acc: 0.7500\n",
      "Epoch 116/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 8.4605e-08 - acc: 1.0000 - val_loss: 4.1533 - val_acc: 0.7333\n",
      "Epoch 117/120\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 7.0465e-08 - acc: 1.0000 - val_loss: 4.1583 - val_acc: 0.7333\n",
      "Epoch 118/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 1.0374e-07 - acc: 1.0000 - val_loss: 4.1325 - val_acc: 0.7333\n",
      "Epoch 119/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 9.3512e-08 - acc: 1.0000 - val_loss: 4.1372 - val_acc: 0.7333\n",
      "Epoch 120/120\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 5.6823e-08 - acc: 1.0000 - val_loss: 4.1554 - val_acc: 0.7333\n",
      "C:\\Users\\shlee\\anaconda3\\envs\\develop\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "# build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# start training the model\n",
    "history = model.fit([Xstrain1, Xqtrain1],\n",
    "         Ytrain1, batch_size, train_epochs,\n",
    "         validation_data=([Xstest1, Xqtest1], Ytest1))\n",
    "\n",
    "# save model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "113/113 [==============================] - 0s 4ms/step - loss: 4.1554 - acc: 0.7333\n",
      "\n",
      " 테스트 정확도: 73.33%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n 테스트 정확도: %.2f%%\" % (model.evaluate([Xstest1, Xqtest1], Ytest1)[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"279.59625pt\" version=\"1.1\" viewBox=\"0 0 424.828125 279.59625\" width=\"424.828125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-06-24T23:07:20.329272</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 279.59625 \r\nL 424.828125 279.59625 \r\nL 424.828125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 117.118125 \r\nL 417.628125 117.118125 \r\nL 417.628125 22.318125 \r\nL 30.103125 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m9bddeb51d5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"47.717898\" xlink:href=\"#m9bddeb51d5\" y=\"117.118125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(44.536648 131.716563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.927218\" xlink:href=\"#m9bddeb51d5\" y=\"117.118125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(100.564718 131.716563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"166.136538\" xlink:href=\"#m9bddeb51d5\" y=\"117.118125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(159.774038 131.716563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"225.345858\" xlink:href=\"#m9bddeb51d5\" y=\"117.118125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(218.983358 131.716563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"284.555178\" xlink:href=\"#m9bddeb51d5\" y=\"117.118125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 80 -->\r\n      <g transform=\"translate(278.192678 131.716563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"343.764498\" xlink:href=\"#m9bddeb51d5\" y=\"117.118125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(334.220748 131.716563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"402.973818\" xlink:href=\"#m9bddeb51d5\" y=\"117.118125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 120 -->\r\n      <g transform=\"translate(393.430068 131.716563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m7d7854bfce\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m7d7854bfce\" y=\"71.986068\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.5 -->\r\n      <g transform=\"translate(7.2 75.785286)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m7d7854bfce\" y=\"26.627216\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(7.2 30.426435)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_10\">\r\n    <path clip-path=\"url(#p74a62026e6)\" d=\"M 47.717898 112.809034 \r\nL 50.678364 104.140454 \r\nL 53.63883 97.58862 \r\nL 56.599296 72.338857 \r\nL 59.559762 45.42594 \r\nL 62.520228 32.347473 \r\nL 65.480694 27.887185 \r\nL 68.44116 26.828812 \r\nL 71.401626 26.702814 \r\nL 74.362092 26.627216 \r\nL 77.322558 26.627216 \r\nL 80.283024 26.627216 \r\nL 83.24349 26.627216 \r\nL 86.203956 26.627216 \r\nL 89.164422 26.627216 \r\nL 92.124888 26.627216 \r\nL 95.085354 26.627216 \r\nL 98.04582 26.627216 \r\nL 101.006286 26.627216 \r\nL 103.966752 26.627216 \r\nL 106.927218 26.627216 \r\nL 109.887684 26.627216 \r\nL 112.84815 26.627216 \r\nL 115.808616 26.627216 \r\nL 118.769082 26.627216 \r\nL 121.729548 26.627216 \r\nL 124.690014 26.627216 \r\nL 127.65048 26.627216 \r\nL 130.610946 26.627216 \r\nL 133.571412 26.627216 \r\nL 136.531878 26.627216 \r\nL 139.492344 26.627216 \r\nL 142.45281 26.627216 \r\nL 145.413276 26.627216 \r\nL 148.373742 26.627216 \r\nL 151.334208 26.627216 \r\nL 154.294674 26.627216 \r\nL 157.25514 26.627216 \r\nL 160.215606 26.627216 \r\nL 163.176072 26.627216 \r\nL 166.136538 26.627216 \r\nL 169.097004 26.627216 \r\nL 172.05747 26.627216 \r\nL 175.017936 26.627216 \r\nL 177.978402 26.627216 \r\nL 180.938868 26.627216 \r\nL 183.899334 26.627216 \r\nL 186.8598 26.627216 \r\nL 189.820266 26.627216 \r\nL 192.780732 26.627216 \r\nL 195.741198 26.627216 \r\nL 198.701664 26.627216 \r\nL 201.66213 26.627216 \r\nL 204.622596 26.627216 \r\nL 207.583062 26.627216 \r\nL 210.543528 26.627216 \r\nL 213.503994 26.627216 \r\nL 216.46446 26.627216 \r\nL 219.424926 26.627216 \r\nL 222.385392 26.627216 \r\nL 225.345858 26.627216 \r\nL 228.306324 26.627216 \r\nL 231.26679 26.627216 \r\nL 234.227256 26.627216 \r\nL 237.187722 26.627216 \r\nL 240.148188 26.627216 \r\nL 243.108654 26.627216 \r\nL 246.06912 26.627216 \r\nL 249.029586 26.627216 \r\nL 251.990052 26.627216 \r\nL 254.950518 26.627216 \r\nL 257.910984 26.627216 \r\nL 260.87145 26.627216 \r\nL 263.831916 26.627216 \r\nL 266.792382 26.627216 \r\nL 269.752848 26.627216 \r\nL 272.713314 26.627216 \r\nL 275.67378 26.627216 \r\nL 278.634246 26.627216 \r\nL 281.594712 26.627216 \r\nL 284.555178 26.627216 \r\nL 287.515644 26.627216 \r\nL 290.47611 26.627216 \r\nL 293.436576 26.627216 \r\nL 296.397042 26.627216 \r\nL 299.357508 26.627216 \r\nL 302.317974 26.627216 \r\nL 305.27844 26.627216 \r\nL 308.238906 26.627216 \r\nL 311.199372 26.627216 \r\nL 314.159838 26.627216 \r\nL 317.120304 26.627216 \r\nL 320.08077 26.627216 \r\nL 323.041236 26.627216 \r\nL 326.001702 26.627216 \r\nL 328.962168 26.627216 \r\nL 331.922634 26.627216 \r\nL 334.8831 26.627216 \r\nL 337.843566 26.627216 \r\nL 340.804032 26.627216 \r\nL 343.764498 26.627216 \r\nL 346.724964 26.627216 \r\nL 349.68543 26.627216 \r\nL 352.645896 26.627216 \r\nL 355.606362 26.627216 \r\nL 358.566828 26.627216 \r\nL 361.527294 26.627216 \r\nL 364.48776 26.627216 \r\nL 367.448226 26.627216 \r\nL 370.408692 26.627216 \r\nL 373.369158 26.627216 \r\nL 376.329624 26.627216 \r\nL 379.29009 26.627216 \r\nL 382.250556 26.627216 \r\nL 385.211022 26.627216 \r\nL 388.171488 26.627216 \r\nL 391.131954 26.627216 \r\nL 394.09242 26.627216 \r\nL 397.052886 26.627216 \r\nL 400.013352 26.627216 \r\n\" style=\"fill:none;stroke:#008000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_11\">\r\n    <path clip-path=\"url(#p74a62026e6)\" d=\"M 47.717898 106.761187 \r\nL 50.678364 103.737263 \r\nL 53.63883 90.129607 \r\nL 56.599296 70.474107 \r\nL 59.559762 53.842528 \r\nL 62.520228 49.306642 \r\nL 65.480694 49.306642 \r\nL 68.44116 49.306642 \r\nL 71.401626 49.306642 \r\nL 74.362092 49.306642 \r\nL 77.322558 49.306642 \r\nL 80.283024 49.306642 \r\nL 83.24349 49.306642 \r\nL 86.203956 49.306642 \r\nL 89.164422 49.306642 \r\nL 92.124888 49.306642 \r\nL 95.085354 50.818602 \r\nL 98.04582 50.818602 \r\nL 101.006286 49.306642 \r\nL 103.966752 49.306642 \r\nL 106.927218 50.818602 \r\nL 109.887684 50.818602 \r\nL 112.84815 49.306642 \r\nL 115.808616 49.306642 \r\nL 118.769082 50.818602 \r\nL 121.729548 50.818602 \r\nL 124.690014 50.818602 \r\nL 127.65048 50.818602 \r\nL 130.610946 50.818602 \r\nL 133.571412 49.306642 \r\nL 136.531878 50.818602 \r\nL 139.492344 50.818602 \r\nL 142.45281 50.818602 \r\nL 145.413276 50.818602 \r\nL 148.373742 47.794681 \r\nL 151.334208 49.306642 \r\nL 154.294674 50.818602 \r\nL 157.25514 50.818602 \r\nL 160.215606 50.818602 \r\nL 163.176072 49.306642 \r\nL 166.136538 50.818602 \r\nL 169.097004 50.818602 \r\nL 172.05747 49.306642 \r\nL 175.017936 47.794681 \r\nL 177.978402 49.306642 \r\nL 180.938868 49.306642 \r\nL 183.899334 49.306642 \r\nL 186.8598 49.306642 \r\nL 189.820266 49.306642 \r\nL 192.780732 50.818602 \r\nL 195.741198 49.306642 \r\nL 198.701664 49.306642 \r\nL 201.66213 49.306642 \r\nL 204.622596 50.818602 \r\nL 207.583062 50.818602 \r\nL 210.543528 50.818602 \r\nL 213.503994 50.818602 \r\nL 216.46446 50.818602 \r\nL 219.424926 50.818602 \r\nL 222.385392 50.818602 \r\nL 225.345858 50.818602 \r\nL 228.306324 50.818602 \r\nL 231.26679 49.306642 \r\nL 234.227256 50.818602 \r\nL 237.187722 49.306642 \r\nL 240.148188 49.306642 \r\nL 243.108654 49.306642 \r\nL 246.06912 50.818602 \r\nL 249.029586 49.306642 \r\nL 251.990052 49.306642 \r\nL 254.950518 50.818602 \r\nL 257.910984 50.818602 \r\nL 260.87145 49.306642 \r\nL 263.831916 49.306642 \r\nL 266.792382 49.306642 \r\nL 269.752848 49.306642 \r\nL 272.713314 47.794681 \r\nL 275.67378 49.306642 \r\nL 278.634246 49.306642 \r\nL 281.594712 49.306642 \r\nL 284.555178 49.306642 \r\nL 287.515644 49.306642 \r\nL 290.47611 49.306642 \r\nL 293.436576 49.306642 \r\nL 296.397042 49.306642 \r\nL 299.357508 49.306642 \r\nL 302.317974 49.306642 \r\nL 305.27844 50.818602 \r\nL 308.238906 50.818602 \r\nL 311.199372 50.818602 \r\nL 314.159838 50.818602 \r\nL 317.120304 50.818602 \r\nL 320.08077 49.306642 \r\nL 323.041236 49.306642 \r\nL 326.001702 49.306642 \r\nL 328.962168 49.306642 \r\nL 331.922634 49.306642 \r\nL 334.8831 49.306642 \r\nL 337.843566 50.818602 \r\nL 340.804032 50.818602 \r\nL 343.764498 50.818602 \r\nL 346.724964 50.818602 \r\nL 349.68543 50.818602 \r\nL 352.645896 50.818602 \r\nL 355.606362 50.818602 \r\nL 358.566828 50.818602 \r\nL 361.527294 50.818602 \r\nL 364.48776 50.818602 \r\nL 367.448226 50.818602 \r\nL 370.408692 50.818602 \r\nL 373.369158 50.818602 \r\nL 376.329624 50.818602 \r\nL 379.29009 50.818602 \r\nL 382.250556 49.306642 \r\nL 385.211022 49.306642 \r\nL 388.171488 50.818602 \r\nL 391.131954 50.818602 \r\nL 394.09242 50.818602 \r\nL 397.052886 50.818602 \r\nL 400.013352 50.818602 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 117.118125 \r\nL 30.103125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 417.628125 117.118125 \r\nL 417.628125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 117.118125 \r\nL 417.628125 117.118125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 22.318125 \r\nL 417.628125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_10\">\r\n    <!-- Accuracy -->\r\n    <g transform=\"translate(196.471875 16.318125)scale(0.12 -0.12)\">\r\n     <defs>\r\n      <path d=\"M 34.1875 63.1875 \r\nL 20.796875 26.90625 \r\nL 47.609375 26.90625 \r\nz\r\nM 28.609375 72.90625 \r\nL 39.796875 72.90625 \r\nL 67.578125 0 \r\nL 57.328125 0 \r\nL 50.6875 18.703125 \r\nL 17.828125 18.703125 \r\nL 11.1875 0 \r\nL 0.78125 0 \r\nz\r\n\" id=\"DejaVuSans-65\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n      <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 32.171875 -5.078125 \r\nQ 28.375 -14.84375 24.75 -17.8125 \r\nQ 21.140625 -20.796875 15.09375 -20.796875 \r\nL 7.90625 -20.796875 \r\nL 7.90625 -13.28125 \r\nL 13.1875 -13.28125 \r\nQ 16.890625 -13.28125 18.9375 -11.515625 \r\nQ 21 -9.765625 23.484375 -3.21875 \r\nL 25.09375 0.875 \r\nL 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 11.921875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-121\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-65\"/>\r\n     <use x=\"66.658203\" xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"121.638672\" xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"176.619141\" xlink:href=\"#DejaVuSans-117\"/>\r\n     <use x=\"239.998047\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"281.111328\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"342.390625\" xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"397.371094\" xlink:href=\"#DejaVuSans-121\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 329.39375 112.118125 \r\nL 410.628125 112.118125 \r\nQ 412.628125 112.118125 412.628125 110.118125 \r\nL 412.628125 81.761875 \r\nQ 412.628125 79.761875 410.628125 79.761875 \r\nL 329.39375 79.761875 \r\nQ 327.39375 79.761875 327.39375 81.761875 \r\nL 327.39375 110.118125 \r\nQ 327.39375 112.118125 329.39375 112.118125 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_12\">\r\n     <path d=\"M 331.39375 87.860313 \r\nL 351.39375 87.860313 \r\n\" style=\"fill:none;stroke:#008000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_13\"/>\r\n    <g id=\"text_11\">\r\n     <!-- train -->\r\n     <g transform=\"translate(359.39375 91.360313)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_14\">\r\n     <path d=\"M 331.39375 102.538438 \r\nL 351.39375 102.538438 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\"/>\r\n    <g id=\"text_12\">\r\n     <!-- validation -->\r\n     <g transform=\"translate(359.39375 106.038438)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n       <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"176.025391\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"239.501953\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"300.78125\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"339.990234\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"367.773438\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"428.955078\" xlink:href=\"#DejaVuSans-110\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n  <g id=\"axes_2\">\r\n   <g id=\"patch_8\">\r\n    <path d=\"M 30.103125 255.718125 \r\nL 417.628125 255.718125 \r\nL 417.628125 160.918125 \r\nL 30.103125 160.918125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_3\">\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"47.717898\" xlink:href=\"#m9bddeb51d5\" y=\"255.718125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(44.536648 270.316563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_9\">\r\n     <g id=\"line2d_17\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.927218\" xlink:href=\"#m9bddeb51d5\" y=\"255.718125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(100.564718 270.316563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_10\">\r\n     <g id=\"line2d_18\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"166.136538\" xlink:href=\"#m9bddeb51d5\" y=\"255.718125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(159.774038 270.316563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_11\">\r\n     <g id=\"line2d_19\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"225.345858\" xlink:href=\"#m9bddeb51d5\" y=\"255.718125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(218.983358 270.316563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_12\">\r\n     <g id=\"line2d_20\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"284.555178\" xlink:href=\"#m9bddeb51d5\" y=\"255.718125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_17\">\r\n      <!-- 80 -->\r\n      <g transform=\"translate(278.192678 270.316563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_13\">\r\n     <g id=\"line2d_21\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"343.764498\" xlink:href=\"#m9bddeb51d5\" y=\"255.718125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_18\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(334.220748 270.316563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_14\">\r\n     <g id=\"line2d_22\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"402.973818\" xlink:href=\"#m9bddeb51d5\" y=\"255.718125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_19\">\r\n      <!-- 120 -->\r\n      <g transform=\"translate(393.430068 270.316563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_4\">\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_23\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m7d7854bfce\" y=\"251.409035\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_20\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(16.740625 255.208254)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_24\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m7d7854bfce\" y=\"209.958662\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_21\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(16.740625 213.75788)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_25\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m7d7854bfce\" y=\"168.508288\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_22\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(16.740625 172.307507)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_26\">\r\n    <path clip-path=\"url(#p192d96dc9b)\" d=\"M 47.717898 173.256058 \r\nL 50.678364 191.579955 \r\nL 53.63883 205.128368 \r\nL 56.599296 217.133719 \r\nL 59.559762 232.175242 \r\nL 62.520228 242.378892 \r\nL 65.480694 247.500308 \r\nL 68.44116 249.925987 \r\nL 71.401626 250.758958 \r\nL 74.362092 251.14636 \r\nL 77.322558 251.290869 \r\nL 80.283024 251.353398 \r\nL 83.24349 251.384495 \r\nL 86.203956 251.39207 \r\nL 89.164422 251.401041 \r\nL 92.124888 251.402693 \r\nL 95.085354 251.402889 \r\nL 98.04582 251.406889 \r\nL 101.006286 251.407667 \r\nL 103.966752 251.407898 \r\nL 106.927218 251.408256 \r\nL 109.887684 251.408494 \r\nL 112.84815 251.408626 \r\nL 115.808616 251.408762 \r\nL 118.769082 251.408834 \r\nL 121.729548 251.408875 \r\nL 124.690014 251.408922 \r\nL 127.65048 251.408941 \r\nL 130.610946 251.408946 \r\nL 133.571412 251.408959 \r\nL 136.531878 251.408977 \r\nL 139.492344 251.408936 \r\nL 142.45281 251.408986 \r\nL 145.413276 251.409001 \r\nL 148.373742 251.408983 \r\nL 151.334208 251.408962 \r\nL 154.294674 251.409014 \r\nL 157.25514 251.409009 \r\nL 160.215606 251.409014 \r\nL 163.176072 251.409018 \r\nL 166.136538 251.409018 \r\nL 169.097004 251.40902 \r\nL 172.05747 251.409022 \r\nL 175.017936 251.40902 \r\nL 177.978402 251.409023 \r\nL 180.938868 251.409021 \r\nL 183.899334 251.409022 \r\nL 186.8598 251.409004 \r\nL 189.820266 251.409026 \r\nL 192.780732 251.409023 \r\nL 195.741198 251.409027 \r\nL 198.701664 251.409026 \r\nL 201.66213 251.409018 \r\nL 204.622596 251.409025 \r\nL 207.583062 251.409029 \r\nL 210.543528 251.409029 \r\nL 213.503994 251.409027 \r\nL 216.46446 251.409032 \r\nL 219.424926 251.409028 \r\nL 222.385392 251.409031 \r\nL 225.345858 251.409029 \r\nL 228.306324 251.409018 \r\nL 231.26679 251.409027 \r\nL 234.227256 251.40903 \r\nL 237.187722 251.409027 \r\nL 240.148188 251.409028 \r\nL 243.108654 251.409018 \r\nL 246.06912 251.409028 \r\nL 249.029586 251.409019 \r\nL 251.990052 251.409031 \r\nL 254.950518 251.409028 \r\nL 257.910984 251.409032 \r\nL 260.87145 251.40902 \r\nL 263.831916 251.409031 \r\nL 266.792382 251.409032 \r\nL 269.752848 251.409032 \r\nL 272.713314 251.40903 \r\nL 275.67378 251.409024 \r\nL 278.634246 251.409033 \r\nL 281.594712 251.409032 \r\nL 284.555178 251.409031 \r\nL 287.515644 251.409032 \r\nL 290.47611 251.409033 \r\nL 293.436576 251.409029 \r\nL 296.397042 251.409031 \r\nL 299.357508 251.409032 \r\nL 302.317974 251.40903 \r\nL 305.27844 251.409026 \r\nL 308.238906 251.409033 \r\nL 311.199372 251.409032 \r\nL 314.159838 251.409033 \r\nL 317.120304 251.409033 \r\nL 320.08077 251.409033 \r\nL 323.041236 251.409032 \r\nL 326.001702 251.409033 \r\nL 328.962168 251.409033 \r\nL 331.922634 251.40903 \r\nL 334.8831 251.409032 \r\nL 337.843566 251.409033 \r\nL 340.804032 251.409033 \r\nL 343.764498 251.409034 \r\nL 346.724964 251.409033 \r\nL 349.68543 251.409033 \r\nL 352.645896 251.409032 \r\nL 355.606362 251.409032 \r\nL 358.566828 251.409033 \r\nL 361.527294 251.409034 \r\nL 364.48776 251.409033 \r\nL 367.448226 251.409034 \r\nL 370.408692 251.409032 \r\nL 373.369158 251.409033 \r\nL 376.329624 251.409033 \r\nL 379.29009 251.409019 \r\nL 382.250556 251.409033 \r\nL 385.211022 251.409034 \r\nL 388.171488 251.409034 \r\nL 391.131954 251.409034 \r\nL 394.09242 251.409033 \r\nL 397.052886 251.409033 \r\nL 400.013352 251.409034 \r\n\" style=\"fill:none;stroke:#008000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_27\">\r\n    <path clip-path=\"url(#p192d96dc9b)\" d=\"M 47.717898 174.353014 \r\nL 50.678364 191.729567 \r\nL 53.63883 197.949637 \r\nL 56.599296 206.489179 \r\nL 59.559762 213.994723 \r\nL 62.520228 216.690682 \r\nL 65.480694 217.233868 \r\nL 68.44116 215.278141 \r\nL 71.401626 213.180036 \r\nL 74.362092 209.563622 \r\nL 77.322558 207.647383 \r\nL 80.283024 204.116683 \r\nL 83.24349 202.744784 \r\nL 86.203956 200.080234 \r\nL 89.164422 197.265385 \r\nL 92.124888 197.214851 \r\nL 95.085354 196.319428 \r\nL 98.04582 194.267294 \r\nL 101.006286 192.662943 \r\nL 103.966752 189.986737 \r\nL 106.927218 190.67973 \r\nL 109.887684 187.707196 \r\nL 112.84815 187.129013 \r\nL 115.808616 186.592806 \r\nL 118.769082 184.872538 \r\nL 121.729548 185.556212 \r\nL 124.690014 183.174065 \r\nL 127.65048 183.226428 \r\nL 130.610946 180.942983 \r\nL 133.571412 180.410117 \r\nL 136.531878 178.901651 \r\nL 139.492344 180.217205 \r\nL 142.45281 177.780506 \r\nL 145.413276 178.963965 \r\nL 148.373742 177.540815 \r\nL 151.334208 176.621823 \r\nL 154.294674 175.779643 \r\nL 157.25514 174.772207 \r\nL 160.215606 174.867252 \r\nL 163.176072 174.783572 \r\nL 166.136538 173.878233 \r\nL 169.097004 174.890555 \r\nL 172.05747 174.014834 \r\nL 175.017936 174.821392 \r\nL 177.978402 174.258493 \r\nL 180.938868 172.290939 \r\nL 183.899334 173.756865 \r\nL 186.8598 173.484537 \r\nL 189.820266 172.504614 \r\nL 192.780732 173.972892 \r\nL 195.741198 172.457188 \r\nL 198.701664 171.789835 \r\nL 201.66213 172.095255 \r\nL 204.622596 171.620008 \r\nL 207.583062 171.091969 \r\nL 210.543528 172.037704 \r\nL 213.503994 171.952659 \r\nL 216.46446 169.220414 \r\nL 219.424926 169.630154 \r\nL 222.385392 169.715593 \r\nL 225.345858 168.977916 \r\nL 228.306324 169.484802 \r\nL 231.26679 170.151977 \r\nL 234.227256 170.595752 \r\nL 237.187722 170.916337 \r\nL 240.148188 169.16312 \r\nL 243.108654 169.926462 \r\nL 246.06912 169.658902 \r\nL 249.029586 169.701516 \r\nL 251.990052 169.304248 \r\nL 254.950518 168.751814 \r\nL 257.910984 169.443122 \r\nL 260.87145 172.338592 \r\nL 263.831916 170.590628 \r\nL 266.792382 169.046264 \r\nL 269.752848 169.145949 \r\nL 272.713314 168.37691 \r\nL 275.67378 169.992389 \r\nL 278.634246 169.223833 \r\nL 281.594712 168.962539 \r\nL 284.555178 170.595396 \r\nL 287.515644 170.700962 \r\nL 290.47611 170.03433 \r\nL 293.436576 168.765891 \r\nL 296.397042 168.244217 \r\nL 299.357508 168.878849 \r\nL 302.317974 168.748355 \r\nL 305.27844 169.937713 \r\nL 308.238906 168.644588 \r\nL 311.199372 167.3302 \r\nL 314.159838 166.962432 \r\nL 317.120304 167.286549 \r\nL 320.08077 167.78874 \r\nL 323.041236 168.173221 \r\nL 326.001702 167.59778 \r\nL 328.962168 166.875119 \r\nL 331.922634 168.347964 \r\nL 334.8831 167.519115 \r\nL 337.843566 166.841608 \r\nL 340.804032 166.244781 \r\nL 343.764498 166.636683 \r\nL 346.724964 166.312032 \r\nL 349.68543 166.287138 \r\nL 352.645896 165.229153 \r\nL 355.606362 165.441983 \r\nL 358.566828 166.468572 \r\nL 361.527294 166.05223 \r\nL 364.48776 165.414006 \r\nL 367.448226 166.302614 \r\nL 370.408692 166.390618 \r\nL 373.369158 166.132061 \r\nL 376.329624 167.243392 \r\nL 379.29009 167.335656 \r\nL 382.250556 165.786429 \r\nL 385.211022 165.72659 \r\nL 388.171488 165.330894 \r\nL 391.131954 165.227216 \r\nL 394.09242 165.761832 \r\nL 397.052886 165.664923 \r\nL 400.013352 165.287005 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path d=\"M 30.103125 255.718125 \r\nL 30.103125 160.918125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path d=\"M 417.628125 255.718125 \r\nL 417.628125 160.918125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_11\">\r\n    <path d=\"M 30.103125 255.718125 \r\nL 417.628125 255.718125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_12\">\r\n    <path d=\"M 30.103125 160.918125 \r\nL 417.628125 160.918125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_23\">\r\n    <!-- Loss -->\r\n    <g transform=\"translate(210.705 154.918125)scale(0.12 -0.12)\">\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 19.671875 72.90625 \r\nL 19.671875 8.296875 \r\nL 55.171875 8.296875 \r\nL 55.171875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-76\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-76\"/>\r\n     <use x=\"53.962891\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"115.144531\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"167.244141\" xlink:href=\"#DejaVuSans-115\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"legend_2\">\r\n    <g id=\"patch_13\">\r\n     <path d=\"M 329.39375 250.718125 \r\nL 410.628125 250.718125 \r\nQ 412.628125 250.718125 412.628125 248.718125 \r\nL 412.628125 220.361875 \r\nQ 412.628125 218.361875 410.628125 218.361875 \r\nL 329.39375 218.361875 \r\nQ 327.39375 218.361875 327.39375 220.361875 \r\nL 327.39375 248.718125 \r\nQ 327.39375 250.718125 329.39375 250.718125 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_28\">\r\n     <path d=\"M 331.39375 226.460313 \r\nL 351.39375 226.460313 \r\n\" style=\"fill:none;stroke:#008000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_29\"/>\r\n    <g id=\"text_24\">\r\n     <!-- train -->\r\n     <g transform=\"translate(359.39375 229.960313)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_30\">\r\n     <path d=\"M 331.39375 241.138438 \r\nL 351.39375 241.138438 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_31\"/>\r\n    <g id=\"text_25\">\r\n     <!-- validation -->\r\n     <g transform=\"translate(359.39375 244.638438)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"176.025391\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"239.501953\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"300.78125\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"339.990234\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"367.773438\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"428.955078\" xlink:href=\"#DejaVuSans-110\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p74a62026e6\">\r\n   <rect height=\"94.8\" width=\"387.525\" x=\"30.103125\" y=\"22.318125\"/>\r\n  </clipPath>\r\n  <clipPath id=\"p192d96dc9b\">\r\n   <rect height=\"94.8\" width=\"387.525\" x=\"30.103125\" y=\"160.918125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# plot accuracy and loss plot\n",
    "plt.subplot(211)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(history.history[\"acc\"], color=\"g\", label=\"train\")\n",
    "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(history.history[\"loss\"], color=\"g\", label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# labels\n",
    "ytest = np.argmax(Ytest1, axis=1)\n",
    "\n",
    "# get predictions\n",
    "Ytest_ = model.predict([Xstest1, Xqtest1])\n",
    "ytest_ = np.argmax(Ytest_, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}